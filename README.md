# Sign-Language-DetectionğŸ¤–ğŸ§â€â™‚ï¸
ğŸ“Œ Project Overview
This project uses computer vision and machine learning to detect and interpret sign language gestures in real-time using a webcam. It aims to bridge the communication gap between the deaf and others by translating sign language into text or sentences. 
In this project there are many different codes one for collecting the data using one hand, collecting the data using two hands, and the final for testing the working of project i.e to run the project.
In this project I have used Teachable Machine platform form for training the datasets and converting them into the models.
The main aim of the project is to bridge the communication between the deaf and normal people.
I have used American Sign language for my project.
Many different python libraries are used.

ğŸ¯ Features
Real-time hand gesture detection
Conversion of sign language to English text
Webcam integration using OpenCV
Trained model for specific ASL (American Sign Language) words
User-friendly interface

ğŸ› ï¸ Tech Stack
Programming Language: Python 3.9
Libraries & Tools: OpenCV, TensorFlow 2.14 ,MediaPipe, NumPy, Matplotlib
IDE: PyCharm
Model Type: CNN / Pre-trained MediaPipe Hand Landmark Model

*Note- Need to install the specified version only.


